{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis\n",
    "\n",
    "1. For this demo we tries to do sentiment polarization analysis on Twitter data.\n",
    "\n",
    "2. There is no label in the collected dataset, so we use a publically available labeled Twitter dataset, Sentiment140 dataset, as the training data, and trained a sentiment classification model. Sentiment140 dataset can be found here: https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis . \n",
    "\n",
    "3. We performed some preprocessing on part of this dataset, and get \n",
    "\n",
    "    1) \"train.csv\" for training\n",
    "\n",
    "    2) \"test.csv\" for testing\n",
    "\n",
    "    3) An unlabeled \"prediction.csv\" (this is part of our collected model) for sentiment prediction. \n",
    "\n",
    "4. All the files needed to run this notebook can be downloaded from https://drive.google.com/drive/folders/1eDYECK9UnDqhuy7KkA6ISK4w8mD18GdQ?usp=sharing\n",
    "\n",
    "\n",
    "5. To run this demo you need to install torchtext: \n",
    "> ```pip3 install twint```\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis\n",
    "\n",
    "6. This demo uses part of the code provide by torchtext from here: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from Tweet import Tweet\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Sentiment Model Definition\n",
    "\n",
    "These following codes defined a torch embedding model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "\n",
    "        # pooled = [batch size, embedding_dim]\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Define the Training, Testing and Predicting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    #convert into float for division\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    senti_prediction = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            pred = torch.round(torch.sigmoid(predictions))\n",
    "            senti_prediction.extend(list(pred.cpu().numpy()))\n",
    "            \n",
    "    return senti_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Setting Parameters  and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameter\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# change this to your own path\n",
    "data_folder = './sentAnalysis_data'\n",
    "\n",
    "# train data path\n",
    "train_path = 'train.csv'\n",
    "\n",
    "# test data path\n",
    "test_path = 'test.csv'\n",
    "\n",
    "# setting data path for prediction csv file\n",
    "predict_path = 'prediction.csv'\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "BATCH_SIZE = 64      \n",
    "N_EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model():\n",
    "\n",
    "    # define the text and label field for model training\n",
    "    TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "    # loading training and testing dataset from train_path and test_path\n",
    "    train_data, test_data = Tweet.splits(TEXT, LABEL, path=data_folder, train = train_path, test= test_path)  \n",
    "    train_data, valid_data = train_data.split(random_state=random.seed(SEED)) \n",
    "    \n",
    "    # build a vocabulary with training data\n",
    "    MAX_VOCAB_SIZE = 25_000\n",
    "    TEXT.build_vocab(train_data,\n",
    "                     max_size=MAX_VOCAB_SIZE,\n",
    "                     vectors=\"glove.6B.100d\",  \n",
    "                     unk_init=torch.Tensor.normal_)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    # define iterators for train, test and validation\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device)\n",
    "\n",
    "    # decide model dimension\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    EMBEDDING_DIM = 100\n",
    "    OUTPUT_DIM = 1\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    \n",
    "    # define the model\n",
    "    model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "    \n",
    "    # load pretrain word emeddigs\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "    # initialize look-up table\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    # optimizer, criterion\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # set model to CPU or GPU version\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    \n",
    "    # start model training\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'Senti-model.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "\n",
    "    \n",
    "    # save the data.dataset class object TEXT for prediction\n",
    "    output_hal = open(\"TEXT.pkl\", 'wb')\n",
    "    str = pickle.dumps(TEXT)\n",
    "    output_hal.write(str)\n",
    "    output_hal.close()\n",
    "\n",
    "    # save the data.dataset class object TEXT for prediction\n",
    "    output_hal = open(\"LABEL.pkl\", 'wb')\n",
    "    str = pickle.dumps(LABEL)\n",
    "    output_hal.write(str)\n",
    "    output_hal.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Test model  accuracy on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model():\n",
    "    model.load_state_dict(torch.load('Senti-model.pt'))\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Sentiment Classification on Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment_classification(data_folder = '', predict_path = ''):\n",
    "    \n",
    "    # define and load class TEXT\n",
    "    TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
    "    with open(\"TEXT.pkl\", 'rb') as file:\n",
    "        TEXT = pickle.loads(file.read())\n",
    "\n",
    "        \n",
    "    # define and load class LABEL\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "    with open(\"LABEL.pkl\", 'rb') as file:\n",
    "        LABEL = pickle.loads(file.read())\n",
    "\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "    EMBEDDING_DIM = 100\n",
    "    OUTPUT_DIM = 1\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "    # define and load the saved model\n",
    "    model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "    model.load_state_dict(torch.load('Senti-model.pt'))\n",
    "\n",
    "\n",
    "    # set predict=true, then then Tweet.splits would set train=None and test = predict_path\n",
    "    # this function return a tuple (test_data, ), so when only predict data is needed\n",
    "    # you need to take the only element out by calling predict_data = predict_data[0]\n",
    "    predict_data = Tweet.splits(TEXT, LABEL, path=data_folder, predict = True, predict_path = predict_path)\n",
    "    predict_data = predict_data[0]\n",
    "\n",
    "\n",
    "    # here if you only give (predict data) as perameter, the initilization of\n",
    "    # bucketIterator will produce some mistakes.\n",
    "    # I guess that is why the original \"predict_sentiment\" function is not writen\n",
    "    # in a parallel way.\n",
    "    # Check the original code to understand this call\n",
    "    predict_iterator,predict_data = data.BucketIterator.splits(\n",
    "        (predict_data,predict_data),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    prediction = predict(model, predict_iterator)\n",
    "    print('Sentiment classification finished')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  7 Call the train, test and Sentiment_classification function to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classification finished\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# set need_Train = True if this is the first time to run this function\n",
    "# set need_Train = False for sentiment classification\n",
    "\n",
    "# It will download some corpus if it is the first time to run this model\n",
    "\n",
    "need_Train = False\n",
    "\n",
    "if need_Train:\n",
    "    train_model()\n",
    "    test_model()\n",
    "else:\n",
    "    Sentiment_classification(data_folder = './sentAnalysis_data', predict_path = 'prediction.csv')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Perform Sentiment Classification on Tweets in the Same Topic and Visualization\n",
    "\n",
    "In this part we perform sentiment classification on tweets that belong to the same topic detected in \"Tweet_Topic_Modeling.ipynb\"\n",
    "\n",
    "We latter visulize positive and negative tweets repectively as word clouds to see whether sentiment polarization has relationshio with the tweet words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classification finished\n",
      "Topic 0: Percentage of negative user is 0.562992, positive is 0.437008\n",
      "Sentiment classification finished\n",
      "Topic 1: Percentage of negative user is 0.562347, positive is 0.437653\n",
      "Sentiment classification finished\n",
      "Topic 2: Percentage of negative user is 0.532646, positive is 0.467354\n",
      "Sentiment classification finished\n",
      "Topic 3: Percentage of negative user is 0.549593, positive is 0.450407\n",
      "Sentiment classification finished\n",
      "Topic 4: Percentage of negative user is 0.573134, positive is 0.426866\n",
      "Sentiment classification finished\n",
      "Topic 5: Percentage of negative user is 0.578680, positive is 0.421320\n",
      "Sentiment classification finished\n",
      "Topic 6: Percentage of negative user is 0.614815, positive is 0.385185\n",
      "Sentiment classification finished\n",
      "Topic 7: Percentage of negative user is 0.522917, positive is 0.477083\n",
      "Sentiment classification finished\n",
      "Topic 8: Percentage of negative user is 0.534884, positive is 0.465116\n",
      "Sentiment classification finished\n",
      "Topic 9: Percentage of negative user is 0.546032, positive is 0.453968\n",
      "saved word cloud to ./word_clouds\n"
     ]
    }
   ],
   "source": [
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from pyecharts.charts import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def count_word_freq(sent):\n",
    "    fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n",
    "    return fdist\n",
    "\n",
    "def get_word_freq(tweets):\n",
    "    fdist = FreqDist()\n",
    "    for s in tweets:\n",
    "        fdist += count_word_freq(s)\n",
    "    return fdist\n",
    "def visualize(fdist, k_topic, polar):\n",
    "    num_words = 30\n",
    "    words = fdist.most_common(num_words)\n",
    "    shapes = [SymbolType.DIAMOND, SymbolType.RECT, SymbolType.ROUND_RECT, SymbolType.ARROW, SymbolType.TRIANGLE]\n",
    "\n",
    "    w = (WordCloud().add(\"\", words, word_size_range=[20, 100], shape=shapes[0]).set_global_opts(\n",
    "        title_opts=opts.TitleOpts(title='Topic_{}_{}'.format(k_topic, polar))).render(\n",
    "        './word_clouds/Topic_{}_{}.html'.format(k_topic, polar)))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    filename = './Topic_tweets/topic_{}.csv'.format(int(i))\n",
    "\n",
    "    df_data = pd.read_csv(filename)\n",
    "    res = Sentiment_classification(data_folder='./', predict_path=filename)\n",
    "    print('Topic %d: Percentage of negative user is %f, positive is %f' %(i, res.count(0)/len(res), res.count(1)/len(res)))\n",
    "    df_res = pd.DataFrame(np.array(res).reshape(-1, 1), columns=['polar'])\n",
    "    df_res['polar'] = df_res['polar'].replace({0: 'neg', 1:'pos'})\n",
    "    df = pd.concat([df_data, df_res], axis=1)\n",
    "\n",
    "    groups = df.groupby(['polar'])\n",
    "    for group in groups:\n",
    "        polar = group[0]\n",
    "        df = group[1]\n",
    "        df = df['tweet'].dropna().tolist()\n",
    "        fdist = get_word_freq(df)\n",
    "        visualize(fdist,i,polar)\n",
    "print('saved word cloud to ./word_clouds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
