{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Community Detection and Visualization\n",
    "\n",
    "\n",
    "For this demo we aim to identify groups of similar users in Twitter network by Attractor, which is a distance dynamics based topology network community detection algorithm. We choose Attractor because it can discover small-sized communities, which is important in this report because we build the Twitter user network based on conversation interaction between users. A conversation may contain only a very small number of users. Based on the community structure identified, we further visualize each community as a \"word cloud\" to show the underlying high-frequency words discussed by people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Attractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attractor:\n",
    "    def __init__(self, G: nx.Graph, cohesion=0.6, max_iter=50, build_j_dist=False, load_neighbors_from_cache=False):\n",
    "        self.G = G\n",
    "        self.cohesion = cohesion\n",
    "        self.max_iter = max_iter\n",
    "        self.communities = []\n",
    "\n",
    "        neighbor_cache_path = \"./attractor_cache/neighbors.npy\"\n",
    "        need_build_j_dist = 'dist' not in G.edges(data=True).__iter__().__next__()[2].keys()\n",
    "\n",
    "        if build_j_dist or need_build_j_dist:\n",
    "            print('Building Jaccard dist matrix...')\n",
    "            j_dist = list(self._jaccard_dist(ebunch=self.G.edges))\n",
    "            for (u, v, d) in j_dist:\n",
    "                self.G[u][v]['dist'] = d\n",
    "        else:\n",
    "            print('The graph already has the Jaccard dist attribute.')\n",
    "\n",
    "        self.common_neighbors = {}\n",
    "        self.exclusive_neighbors_u = {}\n",
    "        self.exclusive_neighbors_v = {}\n",
    "        if load_neighbors_from_cache and os.path.exists(neighbor_cache_path):\n",
    "            print('Loading neighbors from cache...')\n",
    "            neighbors = np.load(neighbor_cache_path, allow_pickle=True)\n",
    "            self.common_neighbors = neighbors[0]\n",
    "            self.exclusive_neighbors_u = neighbors[1]\n",
    "            self.exclusive_neighbors_v = neighbors[2]\n",
    "        else:\n",
    "            print('Finding common and exclusive neighbors...')\n",
    "            for u, v in tqdm(self.G.edges()):\n",
    "                # common neighbors\n",
    "                if u not in self.common_neighbors.keys():\n",
    "                    self.common_neighbors[u] = {}\n",
    "                self.common_neighbors[u][v] = set(nx.common_neighbors(self.G, u, v))\n",
    "\n",
    "                # exclusive neighbors\n",
    "                self.exclusive_neighbors_u[u] = set(self.G.neighbors(u)) - self.common_neighbors[u][v]\n",
    "                self.exclusive_neighbors_v[v] = set(self.G.neighbors(v)) - self.common_neighbors[u][v]\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(neighbor_cache_path)):\n",
    "                os.makedirs(os.path.dirname(neighbor_cache_path))\n",
    "            np.save(neighbor_cache_path, [self.common_neighbors, self.exclusive_neighbors_u, self.exclusive_neighbors_v])\n",
    "\n",
    "        print('Initialization done.')\n",
    "\n",
    "    def _jaccard_dist(self, ebunch):\n",
    "        def predict(G, u, v):\n",
    "            inter_size = len((set(G[u]) | {u}) & set(G[v]) | {v})\n",
    "            union_size = len(set(G[u]) | set(G[v]) | {u, v})\n",
    "            return 1 - inter_size / union_size\n",
    "\n",
    "        if ebunch is None:\n",
    "            ebunch = nx.non_edges(G)\n",
    "        return ((u, v, predict(self.G, u, v)) for u, v in ebunch)\n",
    "\n",
    "    def _calculate_DI(self, u, v, f):\n",
    "        tmp = f(1 - self.G[u][v]['dist'])\n",
    "        DI = -(tmp / self.G.degree(u) + tmp / self.G.degree(v))\n",
    "        return DI\n",
    "\n",
    "    def _calculate_CI(self, u, v, f):\n",
    "        CI = 0\n",
    "        for x in self.common_neighbors[u][v]:\n",
    "            CI += 1 / self.G.degree(u) * f(1 - self.G[x][u]['dist']) * (1 - self.G[x][u]['dist']) + 1 / self.G.degree(v) * f(1 - self.G[x][v]['dist']) * (1 - self.G[x][v]['dist'])\n",
    "        return -CI\n",
    "\n",
    "    def _calculate_EI(self, u, v, f):\n",
    "        EI = 0\n",
    "\n",
    "        def rho(d_xv, cohesion):\n",
    "            return 1 - d_xv if 1 - d_xv >= cohesion else 1 - d_xv - cohesion\n",
    "\n",
    "        for x in self.exclusive_neighbors_u[u]:\n",
    "            EI += 1 / self.G.degree(u) * f(1 - self.G[x][u]['dist']) * rho(self.G[x][u]['dist'], self.cohesion)\n",
    "        for y in self.exclusive_neighbors_v[v]:\n",
    "            EI += 1 / self.G.degree(v) * f(1 - self.G[y][v]['dist']) * rho(self.G[y][v]['dist'], self.cohesion)\n",
    "        return -EI\n",
    "\n",
    "    def clustering(self, attracted_graph_save_path=None):\n",
    "        # Interaction\n",
    "        print('Start interaction...')\n",
    "        no_finish = True\n",
    "        iter = 0\n",
    "        while no_finish and iter < self.max_iter:\n",
    "            no_finish = False\n",
    "            iter += 1\n",
    "            print('Iter {}'.format(iter))\n",
    "            for (u, v, data) in self.G.edges(data=True):\n",
    "                d_e = data['dist']\n",
    "                if 0 < d_e < 1:\n",
    "                    DI = self._calculate_DI(u, v, np.sin)\n",
    "                    CI = self._calculate_CI(u, v, np.sin)\n",
    "                    EI = self._calculate_EI(u, v, np.sin)\n",
    "\n",
    "                    delta_d_e = DI + CI + EI\n",
    "                    if delta_d_e != 0:\n",
    "                        d_e = np.clip(d_e + delta_d_e, 0, 1)\n",
    "                        self.G[u][v]['dist'] = d_e\n",
    "                        no_finish = True\n",
    "        print('End interaction.')\n",
    "\n",
    "        print('Finding communities.')\n",
    "        # Find communities\n",
    "        del_list = []\n",
    "        for (u, v, data) in self.G.edges(data=True):\n",
    "            d_e = data['dist']\n",
    "            if d_e == 1:\n",
    "                del_list.append((u, v))\n",
    "        self.G.remove_edges_from(del_list)\n",
    "\n",
    "        print('Number of clusters: {}.'.format(len(list(nx.connected_components(self.G)))))\n",
    "        for idx, c in enumerate(sorted(nx.connected_components(self.G), key=len, reverse=True)):\n",
    "            self.communities.append(c)\n",
    "\n",
    "        if attracted_graph_save_path is not None:\n",
    "            print('Saving attracted graph at \"{}\"'.format(attracted_graph_save_path))\n",
    "            nx.write_edgelist(self.G, attracted_graph_save_path)\n",
    "\n",
    "        print('Done.')\n",
    "        return self.communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Performing Community Detection\n",
    "\n",
    "To perform community detection on Twitter user network, the input to Attractor is the sampled connected component (see \"twitter_demo.ipynb\") saved in file \"subGraphEdges\". \n",
    "\n",
    "The output is saved in \"Community.result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Jaccard dist matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 377/428640 [00:00<01:53, 3769.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding common and exclusive neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 428640/428640 [01:38<00:00, 4350.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done.\n",
      "Start interaction...\n",
      "Iter 1\n",
      "Iter 2\n",
      "Iter 3\n",
      "Iter 4\n",
      "Iter 5\n",
      "Iter 6\n",
      "Iter 7\n",
      "Iter 8\n",
      "Iter 9\n",
      "Iter 10\n",
      "Iter 11\n",
      "Iter 12\n",
      "Iter 13\n",
      "Iter 14\n",
      "Iter 15\n",
      "Iter 16\n",
      "Iter 17\n",
      "Iter 18\n",
      "Iter 19\n",
      "Iter 20\n",
      "Iter 21\n",
      "Iter 22\n",
      "Iter 23\n",
      "Iter 24\n",
      "Iter 25\n",
      "Iter 26\n",
      "Iter 27\n",
      "Iter 28\n",
      "Iter 29\n",
      "Iter 30\n",
      "Iter 31\n",
      "Iter 32\n",
      "Iter 33\n",
      "Iter 34\n",
      "Iter 35\n",
      "Iter 36\n",
      "Iter 37\n",
      "Iter 38\n",
      "Iter 39\n",
      "Iter 40\n",
      "Iter 41\n",
      "Iter 42\n",
      "Iter 43\n",
      "Iter 44\n",
      "Iter 45\n",
      "Iter 46\n",
      "Iter 47\n",
      "Iter 48\n",
      "Iter 49\n",
      "Iter 50\n",
      "Iter 51\n",
      "Iter 52\n",
      "Iter 53\n",
      "Iter 54\n",
      "Iter 55\n",
      "Iter 56\n",
      "Iter 57\n",
      "Iter 58\n",
      "Iter 59\n",
      "Iter 60\n",
      "Iter 61\n",
      "Iter 62\n",
      "Iter 63\n",
      "Iter 64\n",
      "Iter 65\n",
      "Iter 66\n",
      "Iter 67\n",
      "Iter 68\n",
      "Iter 69\n",
      "Iter 70\n",
      "Iter 71\n",
      "Iter 72\n",
      "Iter 73\n",
      "Iter 74\n",
      "Iter 75\n",
      "Iter 76\n",
      "Iter 77\n",
      "Iter 78\n",
      "Iter 79\n",
      "Iter 80\n",
      "Iter 81\n",
      "Iter 82\n",
      "Iter 83\n",
      "Iter 84\n",
      "Iter 85\n",
      "Iter 86\n",
      "Iter 87\n",
      "Iter 88\n",
      "Iter 89\n",
      "Iter 90\n",
      "Iter 91\n",
      "Iter 92\n",
      "Iter 93\n",
      "Iter 94\n",
      "Iter 95\n",
      "Iter 96\n",
      "Iter 97\n",
      "Iter 98\n",
      "Iter 99\n",
      "Iter 100\n",
      "End interaction.\n",
      "Finding communities.\n",
      "Number of clusters: 3198.\n",
      "Saving attracted graph at \"subGraphEdges.result\"\n",
      "Done.\n",
      "Finded clusters:\n",
      "cluster 0, size 2107\n",
      "cluster 1, size 114\n",
      "cluster 2, size 83\n",
      "cluster 3, size 58\n",
      "cluster 4, size 51\n",
      "cluster 5, size 45\n",
      "cluster 6, size 38\n",
      "cluster 7, size 38\n",
      "cluster 8, size 32\n",
      "cluster 9, size 32\n",
      "cluster 10, size 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "G = nx.read_edgelist(\"subGraphEdges\", nodetype = int)\n",
    "\n",
    "# Printing size of the 10 largest community \n",
    "attractor = Attractor(G, cohesion=1.3, max_iter=100, load_neighbors_from_cache=True)\n",
    "\n",
    "# \"subGraphEdges.result\" is a middle file obtained from \"Twitter_Dataset_Analysis.ipynb\"\n",
    "communities = attractor.clustering(attracted_graph_save_path=\"subGraphEdges.result\")\n",
    "\n",
    "print('Finded clusters:')\n",
    "first_k = 10\n",
    "for idx, c in enumerate(communities):\n",
    "    print('cluster {}, size {}'.format(idx, len(c)))\n",
    "    if idx >= first_k:\n",
    "        break\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visulization high-freqency words in each community\n",
    "\n",
    "In this part, we collect for each user in the network his/her posted tweets about the vaccine, then for each community, we count the freqency of words, and visualize high-freqency word for each community as a word cloud.\n",
    "\n",
    "The word cloud is saved in dir './word_cloud' as html files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Node Content (tweets) Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morris/.local/lib/python3.6/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import pipline_processing\n",
    "from pyecharts.charts import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "extras_stopwords = set(['vaccine','get','take','make','go','give','know','people','like','one','think','many','say', 'would', 'want','good'])\n",
    "STOPWORDS = STOPWORDS | extras_stopwords\n",
    "\n",
    "def generate_node2tweet(raw_file, output_file, G: nx.Graph):\n",
    "    dataframe = pd.read_csv(raw_file)\n",
    "    dataframe = dataframe[['user_id', 'tweet',]].dropna(axis=0, how='any', inplace=False)\n",
    "\n",
    "    node2tweet = {}\n",
    "    for u in G.nodes():\n",
    "        tweets = dataframe['tweet'][dataframe['user_id'] == u]\n",
    "        if tweets.count() > 0:\n",
    "            node2tweet[u] = pipline_processing(tweets,STOPWORDS)\n",
    "\n",
    "    np.save(output_file, node2tweet)\n",
    "    \n",
    "\n",
    "# change your filepath here \n",
    "filename = 'tweets1127_1130.csv' \n",
    "generate_node2tweet(filename, \"./node2tweet.npy\", nx.read_edgelist(\"./Community.result\", nodetype=int))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "\n",
    "\n",
    "# counting word freqency in a sentence\n",
    "def count_word_freq(sentence):\n",
    "    tokenize_sentence = word_tokenize(sentence)\n",
    "    fdist = FreqDist(word.lower() for word in tokenize_sentence)\n",
    "    return fdist\n",
    "\n",
    "# get word frequency in each community\n",
    "def get_community_word_freq(G: nx.Graph, k, node2tweet, load_from_cache=True):\n",
    "    \n",
    "    if load_from_cache:\n",
    "        fdist_communities = list(np.load('fdist_communities.npy', allow_pickle=True))\n",
    "    else:\n",
    "        communities = sorted(nx.connected_components(G), key=len, reverse=True)[:k]\n",
    "        fdist_communities = []\n",
    "        node2sent = {}\n",
    "        \n",
    "        for k, v in node2tweet.items():\n",
    "            node2sent[k] = \" \".join(v)\n",
    "\n",
    "        for c in communities:\n",
    "            fdist_c = FreqDist()\n",
    "            for u in c:\n",
    "                fdist_c += count_word_freq(node2sent[u])\n",
    "            fdist_communities.append(fdist_c)\n",
    "\n",
    "        np.save('fdist_communities.npy', fdist_communities)\n",
    "\n",
    "    return fdist_communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word Cloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check word clouds html in \"./word_clouds\"\n"
     ]
    }
   ],
   "source": [
    "from pyecharts.charts import WordCloud\n",
    "from pyecharts import options as opts\n",
    "from pyecharts.globals import SymbolType\n",
    "from matplotlib import pyplot as plt\n",
    "from pyecharts.render import make_snapshot\n",
    "from snapshot_selenium import snapshot as driver\n",
    "\n",
    "\n",
    "def visualize_communities(fdists):\n",
    "    num_words = 30\n",
    "    shapes = [SymbolType.DIAMOND, SymbolType.RECT, SymbolType.ROUND_RECT, SymbolType.ARROW, SymbolType.TRIANGLE]\n",
    "    for idx, fdist in enumerate(fdists):\n",
    "\n",
    "        words = fdist.most_common(num_words)\n",
    "        w = (WordCloud().add(\"\", words, word_size_range=[20, 100], shape=shapes[0]).set_global_opts(title_opts=opts.TitleOpts(title='Community {}'.format(idx))).render('./word_clouds/Community_{}.html'.format(idx)))\n",
    "        # You can then check html photos in dir ./word_clouds \n",
    "\n",
    "G = nx.read_edgelist(\"./Community.result\", nodetype=int)\n",
    "node2tweet = np.load(\"./node2tweet.npy\", allow_pickle=True).item()\n",
    "\n",
    "fdist_communities = get_community_word_freq(G, 9, node2tweet, load_from_cache=False)\n",
    "visualize_communities(fdist_communities)    \n",
    "print ('Please check word clouds html in \"./word_clouds\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
